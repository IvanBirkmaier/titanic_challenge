{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Used Sources\n",
    "\n",
    "- https://www.kaggle.com/code/kaichinihira/titanic-accuracy-0-83492-top-1-3\n",
    "- https://andrewritchie05.medium.com/a-fresh-approach-to-the-titanic-dataset-1867118cb257\n",
    "- https://www.kaggle.com/code/cdeotte/titanic-wcg-xgboost-0-84688/notebook#Summary-of-Titanic-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports, Settings and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Read data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df_train['Survived'] = np.where(df_train['Survived'] == 1, 0, 1)\n",
    "\n",
    "df_train_len = len(df_train)\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    \"Mr\": \"male\",\n",
    "    \"Mrs\": \"female\",\n",
    "    \"Miss\": \"female\",\n",
    "    \"Mme\": \"female\",\n",
    "    \"Ms\": \"female\",\n",
    "    \"Lady\": \"female\",\n",
    "    \"Mlle\": \"female\",\n",
    "    # \"Dona\": \"female\",\n",
    "    \"Col\": \"male\",\n",
    "    \"Capt\": \"male\",\n",
    "    \"Jonkheer\": \"male\",\n",
    "    \"Master\": \"male\",\n",
    "    \"Don\": \"male\",\n",
    "    \"Rev\": \"male\",\n",
    "    \"Dr\": \"male\",\n",
    "    \"Sir\": \"male\",\n",
    "    \"Major\": \"male\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_gender(title):\n",
    "    if title in gender_mapping:\n",
    "        return gender_mapping[title]\n",
    "    else:\n",
    "        return \"male\"\n",
    "\n",
    "\n",
    "def extract_title_regex(name):\n",
    "    pattern = r\",\\s*([A-Za-z]+)\\.\\s*\"\n",
    "    match = re.search(pattern, name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"Missing\"  # Handle cases with no title\n",
    "\n",
    "\n",
    "def transform_sex(X):\n",
    "    X[\"Title\"] = X[\"Name\"].apply(extract_title_regex)\n",
    "    # print(X[\"Title\"].value_counts())\n",
    "    X[\"Sex\"] = X[\"Title\"].apply(get_gender)\n",
    "    X.drop([\"Title\"], axis=1, inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_sex(df)\n",
    "df_train = transform_sex(df_train)\n",
    "df_test = transform_sex(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Title Extraction and Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Title\"] = df[\"Name\"].map(lambda x: x.split(\", \")[1].split(\". \")[0])\n",
    "df[\"Title\"].replace([\"Mme\", \"Ms\"], \"Mrs\", inplace=True)\n",
    "df[\"Title\"].replace([\"Mlle\"], \"Miss\", inplace=True)\n",
    "df[\"Title\"].replace(\n",
    "    [\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Major\",\n",
    "        \"Dr\",\n",
    "        \"Rev\",\n",
    "        \"Don\",\n",
    "        \"Sir\",\n",
    "        \"the Countess\",\n",
    "        \"Lady\",\n",
    "        \"Dona\",\n",
    "        \"Jonkheer\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Surname and Family Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Surname\"] = df_train[\"Name\"].map(lambda name: name.split(\",\")[0].strip())\n",
    "df[\"Surname\"] = df[\"Name\"].map(lambda name: name.split(\",\")[0].strip())\n",
    "df_train[\"FamilyGroup\"] = df_train[\"Surname\"].map(df[\"Surname\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Child_Group = df_train.loc[\n",
    "    (df_train[\"FamilyGroup\"] >= 2)\n",
    "    & ((df_train[\"Age\"] <= 16) | (df_train[\"Sex\"] == \"female\"))\n",
    "]\n",
    "Female_Child_Group = Female_Child_Group.groupby(\"Surname\")[\"Survived\"].mean()\n",
    "Male_Adult_Group = df_train.loc[\n",
    "    (df_train[\"FamilyGroup\"] >= 2)\n",
    "    & (df_train[\"Age\"] > 16)\n",
    "    & (df_train[\"Sex\"] == \"male\")\n",
    "]\n",
    "Male_Adult_List = Male_Adult_Group.groupby(\"Surname\")[\"Survived\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Dead and Survivor Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dead_list = set(\n",
    "    Female_Child_Group[Female_Child_Group.apply(lambda x: x == 1)].index\n",
    ") | set(Male_Adult_List[Male_Adult_List.apply(lambda x: x == 1)].index)\n",
    "Survived_list = set(\n",
    "    Female_Child_Group[Female_Child_Group.apply(lambda x: x == 0)].index\n",
    ") | set(Male_Adult_List[Male_Adult_List.apply(lambda x: x == 0)].index)\n",
    "\n",
    "df[\"Dead_list\"] = 0\n",
    "df[\"Survived_list\"] = 0\n",
    "\n",
    "df.loc[df[\"Surname\"].isin(Dead_list), \"Dead_list\"] = 1\n",
    "df.loc[df[\"Surname\"].isin(Survived_list), \"Survived_list\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Last_name\"] = df[\"Name\"].apply(lambda x: x.split(\",\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Family Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Family_survival\"] = 0.5\n",
    "for grp, grp_df in df.groupby([\"Last_name\", \"Fare\"]):\n",
    "\n",
    "    if len(grp_df) != 1:\n",
    "        for index, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(index)[\"Survived\"].max()\n",
    "            smin = grp_df.drop(index)[\"Survived\"].min()\n",
    "            passID = row[\"PassengerId\"]\n",
    "\n",
    "            if smax == 1.0:\n",
    "                df.loc[df[\"PassengerId\"] == passID, \"Family_survival\"] = 1\n",
    "            elif smin == 0.0:\n",
    "                df.loc[df[\"PassengerId\"] == passID, \"Family_survival\"] = 0\n",
    "for grp, grp_df in df.groupby(\"Ticket\"):\n",
    "    if len(grp_df) != 1:\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row[\"Family_survival\"] == 0) | (row[\"Family_survival\"] == 0.5):\n",
    "                smax = grp_df.drop(ind)[\"Survived\"].max()\n",
    "                smin = grp_df.drop(ind)[\"Survived\"].min()\n",
    "                passID = row[\"PassengerId\"]\n",
    "                if smax == 1.0:\n",
    "                    df.loc[df[\"PassengerId\"] == passID, \"Family_survival\"] = 1\n",
    "                elif smin == 0.0:\n",
    "                    df.loc[df[\"PassengerId\"] == passID, \"Family_survival\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ticket = df[df[\"Ticket\"].str.match(\"[0-9]+\")].copy()\n",
    "num_ticket_index = num_ticket.index.values.tolist()\n",
    "num_alpha_ticket = df.drop(num_ticket_index).copy()\n",
    "\n",
    "num_ticket[\"Ticket\"] = num_ticket[\"Ticket\"].apply(lambda x: int(x))\n",
    "num_ticket[\"Ticket_cat\"] = 0\n",
    "num_ticket.loc[\n",
    "    (num_ticket[\"Ticket\"] >= 100000) & (num_ticket[\"Ticket\"] < 200000), \"Ticket_cat\"\n",
    "] = 1\n",
    "num_ticket.loc[\n",
    "    (num_ticket[\"Ticket\"] >= 200000) & (num_ticket[\"Ticket\"] < 300000), \"Ticket_cat\"\n",
    "] = 2\n",
    "num_ticket.loc[(num_ticket[\"Ticket\"] >= 300000), \"Ticket_cat\"] = 3\n",
    "\n",
    "num_alpha_ticket[\"Ticket_cat\"] = 4\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"A.+\"), \"Ticket_cat\"] = 5\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"C.+\"), \"Ticket_cat\"] = 6\n",
    "num_alpha_ticket.loc[\n",
    "    num_alpha_ticket[\"Ticket\"].str.match(\"C\\.*A\\.*.+\"), \"Ticket_cat\"\n",
    "] = 7\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"F\\.C.+\"), \"Ticket_cat\"] = 8\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"PC.+\"), \"Ticket_cat\"] = 9\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"S\\.+.+\"), \"Ticket_cat\"] = 10\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"SC.+\"), \"Ticket_cat\"] = 11\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"SOTON.+\"), \"Ticket_cat\"] = 12\n",
    "num_alpha_ticket.loc[num_alpha_ticket[\"Ticket\"].str.match(\"STON.+\"), \"Ticket_cat\"] = 13\n",
    "num_alpha_ticket.loc[\n",
    "    num_alpha_ticket[\"Ticket\"].str.match(\"W\\.*/C.+\"), \"Ticket_cat\"\n",
    "] = 14\n",
    "\n",
    "df = pd.concat([num_ticket, num_alpha_ticket]).sort_values(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Family\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "df[\"Alone\"] = df[\"Family\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "df[\"Family_small\"] = df[\"Family\"].apply(lambda x: 1 if (2 <= x and x < 5) else 0)\n",
    "df[\"Family_middle\"] = df[\"Family\"].apply(lambda x: 1 if (5 <= x < 8) else 0)\n",
    "df[\"Family_big\"] = df[\"Family\"].apply(lambda x: 1 if (8 <= x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fare\"] = df[\"Fare\"].fillna(df.query('Pclass==3 & Embarked==\"S\"')[\"Fare\"].median())\n",
    "\n",
    "filter_condition = (df[\"Pclass\"] == 1) & (df[\"Fare\"] == 0)\n",
    "filtered_data = df[filter_condition]\n",
    "pclass1_median_fare = df[df[\"Pclass\"] == 1][\"Fare\"].median()\n",
    "df.loc[filter_condition, \"Fare\"] = pclass1_median_fare\n",
    "filter_condition = (df[\"Pclass\"] == 2) & (df[\"Fare\"] == 0)\n",
    "filtered_data = df[filter_condition]\n",
    "pclass1_median_fare = df[df[\"Pclass\"] == 2][\"Fare\"].median()\n",
    "df.loc[filter_condition, \"Fare\"] = pclass1_median_fare\n",
    "filter_condition = (df[\"Pclass\"] == 3) & (df[\"Fare\"] == 0)\n",
    "filtered_data = df[filter_condition]\n",
    "pclass1_median_fare = df[df[\"Pclass\"] == 3][\"Fare\"].median()\n",
    "df.loc[filter_condition, \"Fare\"] = pclass1_median_fare\n",
    "df[\"Fare_cat\"] = pd.qcut(df[\"Fare\"], 4, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cabin\"].fillna(\"n\", inplace=True)\n",
    "df[\"Cabin\"] = df[\"Cabin\"].str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Embarked\"].fillna(\"S\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=[\n",
    "        \"Sex\",\n",
    "        \"Pclass\",\n",
    "        \"Title\",\n",
    "        \"Ticket_cat\",\n",
    "        \"Fare_cat\",\n",
    "        \"Cabin\",\n",
    "        \"Embarked\",\n",
    "        \"Family_survival\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Predict Ages for NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df[\n",
    "    [\n",
    "        \"Age\",\n",
    "        \"Alone\",\n",
    "        \"Family_small\",\n",
    "        \"Family_middle\",\n",
    "        \"Family_big\",\n",
    "        \"Sex_female\",\n",
    "        \"Sex_male\",\n",
    "        \"Pclass_1\",\n",
    "        \"Pclass_2\",\n",
    "        \"Pclass_3\",\n",
    "        \"Title_Master\",\n",
    "        \"Title_Miss\",\n",
    "        \"Title_Mr\",\n",
    "        \"Title_Mrs\",\n",
    "        \"Title_Rare\",\n",
    "        \"Ticket_cat_0\",\n",
    "        \"Ticket_cat_1\",\n",
    "        \"Ticket_cat_2\",\n",
    "        \"Ticket_cat_3\",\n",
    "        \"Ticket_cat_4\",\n",
    "        \"Ticket_cat_5\",\n",
    "        \"Ticket_cat_6\",\n",
    "        \"Ticket_cat_7\",\n",
    "        \"Ticket_cat_8\",\n",
    "        \"Ticket_cat_9\",\n",
    "        \"Ticket_cat_10\",\n",
    "        \"Ticket_cat_11\",\n",
    "        \"Ticket_cat_12\",\n",
    "        \"Ticket_cat_13\",\n",
    "        \"Ticket_cat_14\",\n",
    "        \"Fare_cat_0\",\n",
    "        \"Fare_cat_1\",\n",
    "        \"Fare_cat_2\",\n",
    "        \"Fare_cat_3\",\n",
    "        \"Cabin_A\",\n",
    "        \"Cabin_B\",\n",
    "        \"Cabin_C\",\n",
    "        \"Cabin_D\",\n",
    "        \"Cabin_E\",\n",
    "        \"Cabin_F\",\n",
    "        \"Cabin_G\",\n",
    "        \"Cabin_n\",\n",
    "        \"Embarked_C\",\n",
    "        \"Embarked_Q\",\n",
    "        \"Embarked_S\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_age = df_predict[df_predict.Age.notnull()].values\n",
    "unknown_age = df_predict[df_predict.Age.isnull()].values\n",
    "\n",
    "X = known_age[:, 1:]\n",
    "y = known_age[:, 0]\n",
    "\n",
    "rfr = RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=100, n_jobs=-1)\n",
    "rfr.fit(X, y)\n",
    "predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "\n",
    "df.loc[df.Age.isnull(), \"Age\"] = predictedAges\n",
    "df[\"Age_cat\"] = pd.qcut(df[\"Age\"], q=5, labels=False, precision=1)\n",
    "df = pd.get_dummies(df, columns=[\"Age_cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Surname</th>\n",
       "      <th>Dead_list</th>\n",
       "      <th>...</th>\n",
       "      <th>Family_survival_0.0</th>\n",
       "      <th>Family_survival_0.5</th>\n",
       "      <th>Family_survival_1.0</th>\n",
       "      <th>Age_cat_0</th>\n",
       "      <th>Age_cat_1</th>\n",
       "      <th>Age_cat_2</th>\n",
       "      <th>Age_cat_3</th>\n",
       "      <th>Age_cat_4</th>\n",
       "      <th>lname</th>\n",
       "      <th>fname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Braund</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Braund</td>\n",
       "      <td>Owen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Heikkinen</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Heikkinen</td>\n",
       "      <td>Laina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Futrelle</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Futrelle</td>\n",
       "      <td>Jacques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Allen</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Allen</td>\n",
       "      <td>William</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived                                               Name  \\\n",
       "0            1       1.0                            Braund, Mr. Owen Harris   \n",
       "1            2       0.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       0.0                             Heikkinen, Miss. Laina   \n",
       "3            4       0.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       1.0                           Allen, Mr. William Henry   \n",
       "\n",
       "    Age  SibSp  Parch            Ticket     Fare    Surname  Dead_list  ...  \\\n",
       "0  22.0      1      0         A/5 21171   7.2500     Braund          1  ...   \n",
       "1  38.0      1      0          PC 17599  71.2833    Cumings          0  ...   \n",
       "2  26.0      0      0  STON/O2. 3101282   7.9250  Heikkinen          0  ...   \n",
       "3  35.0      1      0            113803  53.1000   Futrelle          1  ...   \n",
       "4  35.0      0      0            373450   8.0500      Allen          1  ...   \n",
       "\n",
       "   Family_survival_0.0 Family_survival_0.5  Family_survival_1.0  Age_cat_0  \\\n",
       "0                False                True                False      False   \n",
       "1                False                True                False      False   \n",
       "2                False                True                False      False   \n",
       "3                False               False                 True      False   \n",
       "4                False                True                False      False   \n",
       "\n",
       "   Age_cat_1  Age_cat_2  Age_cat_3  Age_cat_4      lname    fname  \n",
       "0       True      False      False      False     Braund     Owen  \n",
       "1      False      False       True      False    Cumings     John  \n",
       "2      False       True      False      False  Heikkinen    Laina  \n",
       "3      False      False       True      False   Futrelle  Jacques  \n",
       "4      False      False       True      False      Allen  William  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new df with the split string and then remove the comma from last name\n",
    "name_df = df['Name'].str.split(\" \", n = -1, expand = True)\n",
    "name_df[0] = name_df[0].replace(',','', regex=True)\n",
    "  \n",
    "# Add the first and last name columns to the train set\n",
    "df['lname']= name_df[0]\n",
    "df['fname']= name_df[2]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethnicolr import pred_wiki_name\n",
    "\n",
    "df_race_pred = df.copy()\n",
    "\n",
    "df_race_pred = pred_wiki_name(df_race_pred, 'lname', 'fname')\n",
    "race_dict = {lname + ' ' + fname: race for lname, fname, race in zip(df_race_pred['lname'], df_race_pred['fname'], df_race_pred['race'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "GreaterEuropean,British                  775\n",
       "GreaterEuropean,WestEuropean,Nordic      110\n",
       "GreaterEuropean,Jewish                    94\n",
       "GreaterEuropean,WestEuropean,French       72\n",
       "GreaterEuropean,WestEuropean,Germanic     52\n",
       "GreaterEuropean,EastEuropean              44\n",
       "GreaterAfrican,Muslim                     42\n",
       "GreaterEuropean,WestEuropean,Italian      36\n",
       "GreaterEuropean,WestEuropean,Hispanic     35\n",
       "GreaterAfrican,Africans                   18\n",
       "Asian,IndianSubContinent                  14\n",
       "Asian,GreaterEastAsian,EastAsian          13\n",
       "Asian,GreaterEastAsian,Japanese            3\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate over df and assign race based on lname + ' ' + fname\n",
    "df['race'] = df['lname'] + ' ' + df['fname']\n",
    "df['race'] = df['race'].map(race_dict)\n",
    "\n",
    "# Check for NaN values in the race column\n",
    "display(df['race'].value_counts())\n",
    "display(df['race'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in test['race'] appear in train['race']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "race\n",
       "Asian,GreaterEastAsian,EastAsian         0.400000\n",
       "Asian,GreaterEastAsian,Japanese          0.666667\n",
       "Asian,IndianSubContinent                 0.750000\n",
       "GreaterAfrican,Africans                  0.363636\n",
       "GreaterAfrican,Muslim                    0.593750\n",
       "GreaterEuropean,British                  0.574803\n",
       "GreaterEuropean,EastEuropean             0.843750\n",
       "GreaterEuropean,Jewish                   0.590164\n",
       "GreaterEuropean,WestEuropean,French      0.581818\n",
       "GreaterEuropean,WestEuropean,Germanic    0.687500\n",
       "GreaterEuropean,WestEuropean,Hispanic    0.695652\n",
       "GreaterEuropean,WestEuropean,Italian     0.677419\n",
       "GreaterEuropean,WestEuropean,Nordic      0.800000\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = df.iloc[:df_train_len]\n",
    "test = df.iloc[df_train_len:]\n",
    "\n",
    "missing_values = test[~test['race'].isin(train['race'])]['race']\n",
    "if not missing_values.empty:\n",
    "    print(\"Values in test['race'] that don't appear in train['race']:\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"All values in test['race'] appear in train['race']\")\n",
    "\n",
    "survival_rate = train.groupby('race')['Survived'].mean()\n",
    "display(survival_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_race_survival(row):\n",
    "    if row is np.nan:\n",
    "        return \"high\"\n",
    "    if survival_rate[row] <= 0.4:\n",
    "        return \"low\"\n",
    "    elif survival_rate[row] <= 0.6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "# df[\"race\"] = df[\"race\"].apply(lambda x: map_race_survival(x))\n",
    "# df[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_groups = {\n",
    "    'GreaterEuropean,British': 'Europe',\n",
    "    'GreaterEuropean,WestEuropean,Nordic': 'Europe',\n",
    "    'GreaterEuropean,Jewish': 'Europe',\n",
    "    'GreaterEuropean,WestEuropean,French': 'Europe',\n",
    "    'GreaterEuropean,WestEuropean,Germanic': 'Europe',\n",
    "    'GreaterEuropean,EastEuropean': 'Europe',\n",
    "    'GreaterAfrican,Muslim': 'Africa',\n",
    "    'GreaterEuropean,WestEuropean,Italian': 'Europe',\n",
    "    'GreaterEuropean,WestEuropean,Hispanic': 'Europe',\n",
    "    'GreaterAfrican,Africans': 'Africa',\n",
    "    'Asian,IndianSubContinent': 'Asia',\n",
    "    'Asian,GreaterEastAsian,EastAsian': 'Asia',\n",
    "    'Asian,GreaterEastAsian,Japanese': 'Asia',\n",
    "}\n",
    "\n",
    "# Apply mapping to create a new 'Continent' column \n",
    "df['race'] = df['race'].map(race_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race_Europe', 'race_Africa', 'race_Asia']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ethnicity_columns = []\n",
    "\n",
    "for ethnicity in df['race'].unique():\n",
    "    if ethnicity is not np.nan:\n",
    "        Ethnicity_columns.append(f\"race_{ethnicity}\")\n",
    "\n",
    "Ethnicity_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=[\n",
    "        \"race\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Name\", \"lname\", \"fname\", \"Last_name\", \"Ticket\", \"Cabin_T\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Create combination columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sex_columns = [\"Sex_male\", \"Sex_female\"]\n",
    "Pclass_columns = [\"Pclass_1\", \"Pclass_2\", \"Pclass_3\"]\n",
    "Age_columns = [\"Age_cat_0\", \"Age_cat_1\", \"Age_cat_2\", \"Age_cat_3\", \"Age_cat_4\"]\n",
    "Family_columns = [\"Alone\", \"Family_small\", \"Family_middle\", \"Family_big\"]\n",
    "Survived_columns = [\"Survived_list\"]\n",
    "Dead_columns = [\"Dead_list\"]\n",
    "\n",
    "\n",
    "def AND(data_frame, columns_1, columns_2):\n",
    "    new_columns = {}\n",
    "    combinations = list(itertools.product(columns_1, columns_2))\n",
    "    for combo in combinations:\n",
    "        new_column_name = \" AND \".join(combo)\n",
    "        new_columns[new_column_name] = (\n",
    "            data_frame[combo[0]].fillna(0) & data_frame[combo[1]].fillna(0)\n",
    "        ).astype(int)\n",
    "    return pd.DataFrame(new_columns)\n",
    "\n",
    "\n",
    "df = pd.concat([df, AND(df, Sex_columns, Family_columns)], axis=1)\n",
    "df = pd.concat([df, AND(df, Pclass_columns, Sex_columns)], axis=1)\n",
    "df = pd.concat([df, AND(df, Age_columns, Sex_columns)], axis=1)\n",
    "df = pd.concat([df, AND(df, Ethnicity_columns, Sex_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Age', 'SibSp', 'Parch', 'Fare', 'Surname',\n",
       "       'Dead_list', 'Survived_list', 'Family', 'Alone', 'Family_small',\n",
       "       'Family_middle', 'Family_big', 'Sex_female', 'Sex_male', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Title_Master', 'Title_Miss', 'Title_Mr',\n",
       "       'Title_Mrs', 'Title_Rare', 'Ticket_cat_0', 'Ticket_cat_1',\n",
       "       'Ticket_cat_2', 'Ticket_cat_3', 'Ticket_cat_4', 'Ticket_cat_5',\n",
       "       'Ticket_cat_6', 'Ticket_cat_7', 'Ticket_cat_8', 'Ticket_cat_9',\n",
       "       'Ticket_cat_10', 'Ticket_cat_11', 'Ticket_cat_12', 'Ticket_cat_13',\n",
       "       'Ticket_cat_14', 'Fare_cat_0', 'Fare_cat_1', 'Fare_cat_2', 'Fare_cat_3',\n",
       "       'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F',\n",
       "       'Cabin_G', 'Cabin_n', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
       "       'Family_survival_0.0', 'Family_survival_0.5', 'Family_survival_1.0',\n",
       "       'Age_cat_0', 'Age_cat_1', 'Age_cat_2', 'Age_cat_3', 'Age_cat_4',\n",
       "       'race_Africa', 'race_Asia', 'race_Europe', 'Sex_male AND Alone',\n",
       "       'Sex_male AND Family_small', 'Sex_male AND Family_middle',\n",
       "       'Sex_male AND Family_big', 'Sex_female AND Alone',\n",
       "       'Sex_female AND Family_small', 'Sex_female AND Family_middle',\n",
       "       'Sex_female AND Family_big', 'Pclass_1 AND Sex_male',\n",
       "       'Pclass_1 AND Sex_female', 'Pclass_2 AND Sex_male',\n",
       "       'Pclass_2 AND Sex_female', 'Pclass_3 AND Sex_male',\n",
       "       'Pclass_3 AND Sex_female', 'Age_cat_0 AND Sex_male',\n",
       "       'Age_cat_0 AND Sex_female', 'Age_cat_1 AND Sex_male',\n",
       "       'Age_cat_1 AND Sex_female', 'Age_cat_2 AND Sex_male',\n",
       "       'Age_cat_2 AND Sex_female', 'Age_cat_3 AND Sex_male',\n",
       "       'Age_cat_3 AND Sex_female', 'Age_cat_4 AND Sex_male',\n",
       "       'Age_cat_4 AND Sex_female', 'race_Europe AND Sex_male',\n",
       "       'race_Europe AND Sex_female', 'race_Africa AND Sex_male',\n",
       "       'race_Africa AND Sex_female', 'race_Asia AND Sex_male',\n",
       "       'race_Asia AND Sex_female'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Preparation for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df[\n",
    "    [\n",
    "        \"Survived\",\n",
    "        \"Dead_list\",\n",
    "        \"Survived_list\",\n",
    "        \"Alone\",\n",
    "        \"Family_small\",\n",
    "        \"Family_middle\",\n",
    "        \"Family_big\",\n",
    "        \"Sex_female\",\n",
    "        \"Sex_male\",\n",
    "        \"Pclass_1\",\n",
    "        \"Pclass_2\",\n",
    "        \"Pclass_3\",\n",
    "        \"Ticket_cat_0\",\n",
    "        \"Ticket_cat_1\",\n",
    "        \"Ticket_cat_2\",\n",
    "        \"Ticket_cat_3\",\n",
    "        \"Ticket_cat_4\",\n",
    "        \"Ticket_cat_5\",\n",
    "        \"Ticket_cat_6\",\n",
    "        \"Ticket_cat_7\",\n",
    "        \"Ticket_cat_8\",\n",
    "        \"Ticket_cat_9\",\n",
    "        \"Ticket_cat_10\",\n",
    "        \"Ticket_cat_11\",\n",
    "        \"Ticket_cat_12\",\n",
    "        \"Ticket_cat_13\",\n",
    "        \"Ticket_cat_14\",\n",
    "        \"Cabin_A\",\n",
    "        \"Cabin_B\",\n",
    "        \"Cabin_C\",\n",
    "        \"Cabin_D\",\n",
    "        \"Cabin_E\",\n",
    "        \"Cabin_F\",\n",
    "        \"Cabin_G\",\n",
    "        \"Cabin_n\",\n",
    "        \"Family_survival_0.0\",\n",
    "        \"Family_survival_0.5\",\n",
    "        \"Family_survival_1.0\",\n",
    "        \"Age_cat_0\",\n",
    "        \"Age_cat_1\",\n",
    "        \"Age_cat_2\",\n",
    "        \"Age_cat_3\",\n",
    "        \"Age_cat_4\",\n",
    "        \"Sex_male AND Alone\",\n",
    "        \"Sex_male AND Family_small\",\n",
    "        \"Sex_male AND Family_middle\",\n",
    "        \"Sex_male AND Family_big\",\n",
    "        \"Sex_female AND Alone\",\n",
    "        \"Sex_female AND Family_small\",\n",
    "        \"Sex_female AND Family_middle\",\n",
    "        \"Sex_female AND Family_big\",\n",
    "        \"Pclass_1 AND Sex_male\",\n",
    "        \"Pclass_1 AND Sex_female\",\n",
    "        \"Pclass_2 AND Sex_male\",\n",
    "        \"Pclass_2 AND Sex_female\",\n",
    "        \"Pclass_3 AND Sex_male\",\n",
    "        \"Pclass_3 AND Sex_female\",\n",
    "        \"Age_cat_0 AND Sex_male\",\n",
    "        \"Age_cat_0 AND Sex_female\",\n",
    "        \"Age_cat_1 AND Sex_male\",\n",
    "        \"Age_cat_1 AND Sex_female\",\n",
    "        \"Age_cat_2 AND Sex_male\",\n",
    "        \"Age_cat_2 AND Sex_female\",\n",
    "        \"Age_cat_3 AND Sex_male\",\n",
    "        \"Age_cat_3 AND Sex_female\",\n",
    "        \"Age_cat_4 AND Sex_male\",\n",
    "        \"Age_cat_4 AND Sex_female\",\n",
    "        \"race_Europe AND Sex_male\",\n",
    "        \"race_Europe AND Sex_female\",\n",
    "        \"race_Africa AND Sex_male\",\n",
    "        \"race_Africa AND Sex_female\",\n",
    "        \"race_Asia AND Sex_male\",\n",
    "        \"race_Asia AND Sex_female\",\n",
    "    ]\n",
    "]\n",
    "train = df_predict.iloc[:df_train_len]\n",
    "test = df_predict.iloc[df_train_len:]\n",
    "X_train = train.drop([\"Survived\"], axis=1)\n",
    "y_train = train[\"Survived\"]\n",
    "X_test = test.drop([\"Survived\"], axis=1)\n",
    "\n",
    "df_test.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def retrain_with_confident_predictions(X_train, y_train, X_test):\\n    confident_survivors = X_test[(X_test[\"Survived_list\"] == True) & (X_test[\"Dead_list\"] == False)]\\n\\n    confident_nonsurvivors = X_test[(X_test[\"Dead_list\"] == True) & (X_test[\"Survived_list\"] == False)]\\n\\n    if not confident_survivors.empty:\\n        X_test = X_test.drop(confident_survivors.index)\\n        X_train = pd.concat([X_train, confident_survivors])\\n        y_train = pd.concat([y_train, pd.Series([1] * len(confident_survivors))])\\n\\n    if not confident_nonsurvivors.empty:\\n        X_test = X_test.drop(confident_nonsurvivors.index)\\n        X_train = pd.concat([X_train, confident_nonsurvivors])\\n        y_train = pd.concat([y_train, pd.Series([0] * len(confident_nonsurvivors))])\\n\\n    return X_train, y_train, X_test, confident_survivors, confident_nonsurvivors\\n\\n\\nX_train, y_train, X_test, confident_survivors, confident_nonsurvivors = retrain_with_confident_predictions(X_train.copy(), y_train.copy(), X_test.copy())'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def retrain_with_confident_predictions(X_train, y_train, X_test):\n",
    "    confident_survivors = X_test[(X_test[\"Survived_list\"] == True) & (X_test[\"Dead_list\"] == False)]\n",
    "\n",
    "    confident_nonsurvivors = X_test[(X_test[\"Dead_list\"] == True) & (X_test[\"Survived_list\"] == False)]\n",
    "\n",
    "    if not confident_survivors.empty:\n",
    "        X_test = X_test.drop(confident_survivors.index)\n",
    "        X_train = pd.concat([X_train, confident_survivors])\n",
    "        y_train = pd.concat([y_train, pd.Series([1] * len(confident_survivors))])\n",
    "\n",
    "    if not confident_nonsurvivors.empty:\n",
    "        X_test = X_test.drop(confident_nonsurvivors.index)\n",
    "        X_train = pd.concat([X_train, confident_nonsurvivors])\n",
    "        y_train = pd.concat([y_train, pd.Series([0] * len(confident_nonsurvivors))])\n",
    "\n",
    "    return X_train, y_train, X_test, confident_survivors, confident_nonsurvivors\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, confident_survivors, confident_nonsurvivors = retrain_with_confident_predictions(X_train.copy(), y_train.copy(), X_test.copy())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Standardize the data (important for PCA)\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Create the PCA object \\npca = PCA(n_components=0.95)  # Keep 95% of the explained variance\\n\\n# Fit and transform the training data\\nX_train_pca = pca.fit_transform(X_train_scaled)\\n\\n# Transform the test data\\nX_test_pca = pca.transform(X_test_scaled)\\n\\nX_train = X_train_pca\\nX_test = X_test_pca'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the PCA object \n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the explained variance\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "X_train = X_train_pca\n",
    "X_test = X_test_pca\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 0.001, 'min_samples_split': 0.01, 'n_estimators': 85}\n",
      "CV Score: 0.864\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [75, 80, 85, 90, 95, 100],\n",
    "    \"max_depth\": [6, 7, 8, 9],\n",
    "    \"min_samples_leaf\": [0.001, 0.01, 0.1],\n",
    "    \"min_samples_split\": [0.001, 0.01, 0.1],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rfc_gs = GridSearchCV(\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "rfc_gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best Parameters: {}\".format(rfc_gs.best_params_))\n",
    "print(\"CV Score: {}\".format(round(rfc_gs.best_score_, 3)))\n",
    "\n",
    "\n",
    "rfc_pred = rfc_gs.predict_proba(X_test)[:, 1]\n",
    "df_test[\"Survived\"] = np.where(rfc_pred >= 0.42, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Reconstruct df_test \\ndf_test[\"Survived_temp\"] = -1  # Placeholder\\n\\n# Update with confident predictions (Example - adjust based on your logic)\\ndf_test.loc[confident_survivors.index, \"Survived_temp\"] = 1\\ndf_test.loc[confident_nonsurvivors.index, \"Survived_temp\"] = 0\\n\\n# Update with regular predictions\\ndf_test.loc[df_test[\\'Survived_temp\\'] == -1, \\'Survived_temp\\'] = np.where(rfc_pred >= 0.42, 0, 1)\\n\\n# Replace column\\ndf_test[\"Survived\"] = df_test[\"Survived_temp\"]\\ndf_test = df_test.drop(\\'Survived_temp\\', axis=1)  # Drop temporary column'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Reconstruct df_test \n",
    "df_test[\"Survived_temp\"] = -1  # Placeholder\n",
    "\n",
    "# Update with confident predictions (Example - adjust based on your logic)\n",
    "df_test.loc[confident_survivors.index, \"Survived_temp\"] = 1\n",
    "df_test.loc[confident_nonsurvivors.index, \"Survived_temp\"] = 0\n",
    "\n",
    "# Update with regular predictions\n",
    "df_test.loc[df_test['Survived_temp'] == -1, 'Survived_temp'] = np.where(rfc_pred >= 0.42, 0, 1)\n",
    "\n",
    "# Replace column\n",
    "df_test[\"Survived\"] = df_test[\"Survived_temp\"]\n",
    "df_test = df_test.drop('Survived_temp', axis=1)  # Drop temporary column\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test[[\"PassengerId\", \"Survived\"]]\n",
    "submission.to_csv(\n",
    "    \"submission_test.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Test the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8133971291866029"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt = pd.read_csv('test.csv', index_col=0)\n",
    "titanic_full = pd.read_csv('titanic_full.csv', skipfooter=1, engine='python')\n",
    "titanic_full.columns = titanic_full.columns.str.capitalize()\n",
    "\n",
    "# fill up survived column in test data based on name\n",
    "Xt['Survived'] = np.NaN\n",
    "for i in range(len(Xt)):\n",
    "    for j in range(len(titanic_full)):\n",
    "        if Xt['Name'][892+i].replace('\"', '') == titanic_full['Name'][j].replace('\"', ''):\n",
    "            Xt['Survived'][892+i] = titanic_full['Survived'][j]\n",
    "            \n",
    "accuracy = (np.where(Xt['Survived'] == 1, 1, 0) == df_test['Survived']).sum() / len(Xt)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEVP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
