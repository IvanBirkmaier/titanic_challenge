{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell:  XGBoost-Klassifikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n",
      "Accuracy scores for the 10 folds: [0.9        0.86666667 0.78333333 0.8        0.85       0.73333333\n",
      " 0.86440678 0.94915254 0.79661017 0.76271186 0.81355932 0.83050847\n",
      " 0.79661017 0.77966102 0.88135593]\n",
      "Average accuracy: 0.8271939736346515\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# Angenommen, deine Trainings- und Testdatensätze wurden bereits geladen in `train` und `test`\n",
    "\n",
    "# Setze 'Survived' im Testdatensatz auf -1, um die Struktur beizubehalten\n",
    "test['Survived'] = -1\n",
    "\n",
    "# Entferne die 'Ticket' Spalte aus beiden Datensätzen\n",
    "# train = train.drop(['Ticket'], axis=1)\n",
    "# test = test.drop(['Ticket'], axis=1)\n",
    "\n",
    "# Definiere Features und Label\n",
    "X_train = train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vorbereitung des Modells\n",
    "# Hier verwenden wir XGBClassifier direkt, da XGBoost seine eigene Methode zur Handhabung von Feature-Skalierung hat,\n",
    "# und es ist nicht zwingend notwendig, StandardScaler in der Pipeline zu verwenden.\n",
    "model = XGBClassifier(n_estimators=1000, random_state=7, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Cross-Validation\n",
    "cv = KFold(n_splits=15, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy scores for the 10 folds: {scores}')\n",
    "print(f'Average accuracy: {scores.mean()}')\n",
    "\n",
    "# Trainiere das Modell mit dem gesamten Trainingsdatensatz\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen für den Testdatensatz\n",
    "X_test = test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Füge Vorhersagen in den Testdatensatz ein und exportiere in CSV\n",
    "test['Survived'] = predictions\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path / '../csv/submission_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for the 15 folds: [0.76666667 0.76666667 0.75       0.8        0.83333333 0.66666667\n",
      " 0.86440678 0.86440678 0.83050847 0.72881356 0.76271186 0.79661017\n",
      " 0.69491525 0.83050847 0.89830508]\n",
      "Average accuracy: 0.7903013182674201\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n",
    "\n",
    "\n",
    "# Definiere Features und Label\n",
    "X_train = train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Setze 'Survived' im Testdatensatz auf -1, um die Struktur beizubehalten\n",
    "test['Survived'] = -1\n",
    "\n",
    "# Vorbereitung des Modells\n",
    "# Hier ersetzen wir XGBClassifier durch DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=7, criterion='gini')\n",
    "\n",
    "# Cross-Validation\n",
    "cv = KFold(n_splits=15, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy scores for the 15 folds: {scores}')\n",
    "print(f'Average accuracy: {scores.mean()}')\n",
    "\n",
    "# Trainiere das Modell mit dem gesamten Trainingsdatensatz\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen für den Testdatensatz\n",
    "X_test = test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Füge Vorhersagen in den Testdatensatz ein und exportiere in CSV\n",
    "test['Survived'] = predictions\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path / '../csv/submission_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n",
      "Accuracy scores for the 15 folds: [0.76666667 0.76666667 0.75       0.8        0.83333333 0.66666667\n",
      " 0.86440678 0.86440678 0.81355932 0.76271186 0.76271186 0.79661017\n",
      " 0.69491525 0.83050847 0.89830508]\n",
      "Average accuracy: 0.7914312617702449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import für den Random Forest Klassifikator\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei, die später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# Angenommen, deine Trainings- und Testdatensätze wurden bereits geladen in `train` und `test`\n",
    "\n",
    "# Setze 'Survived' im Testdatensatz auf -1, um die Struktur beizubehalten\n",
    "test['Survived'] = -1\n",
    "\n",
    "# Definiere Features und Label\n",
    "X_train = train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Vorbereitung des Modells\n",
    "# Random Forest Klassifikator wird hier initialisiert\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=7)\n",
    "\n",
    "# Cross-Validation\n",
    "cv = KFold(n_splits=15, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy scores for the 15 folds: {scores}')\n",
    "print(f'Average accuracy: {scores.mean()}')\n",
    "\n",
    "# Trainiere das Modell mit dem gesamten Trainingsdatensatz\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen für den Testdatensatz\n",
    "X_test = test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Füge Vorhersagen in den Testdatensatz ein und exportiere in CSV\n",
    "test['Survived'] = predictions\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path / '../csv/submission_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_decision_forests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfdf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_decision_forests'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "data_path = Path.cwd()\n",
    "\n",
    "# Lade die Daten\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n",
    "\n",
    "# Zeige die Länge der Datensätze\n",
    "print(len(train), len(test))\n",
    "\n",
    "# Definiere Features und Zielvariable\n",
    "X_train = train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Für TensorFlow Decision Forests müssen wir einen TensorFlow Dataset erstellen\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(X_train, label=\"Survived\")\n",
    "\n",
    "# Erstelle und trainiere das Modell\n",
    "model = tfdf.keras.RandomForestModel()\n",
    "model.fit(train_ds)\n",
    "\n",
    "# Mache Vorhersagen für den Testdatensatz\n",
    "# Hinweis: TF-DF erwartet ebenfalls ein tf.data.Dataset für Vorhersagen. Da wir jedoch die 'Survived'-Spalte im Testdatensatz nicht haben,\n",
    "# entfernen wir sie zusammen mit 'PassengerId', um das Dataset für Vorhersagen zu erstellen.\n",
    "X_test = test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(X_test, task=tfdf.keras.Task.CLASSIFICATION)\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "# TensorFlow Decision Forests gibt Wahrscheinlichkeiten zurück. Um die Klasse zu bestimmen, wählen wir die Klasse mit der höchsten Wahrscheinlichkeit\n",
    "predicted_classes = [int(prediction[0] > 0.5) for prediction in predictions]\n",
    "\n",
    "# Füge Vorhersagen in den Testdatensatz ein und exportiere in CSV\n",
    "test['Survived'] = predicted_classes\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path / '../csv/submission_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Auswahl der wichtigsten Features für ein Modell, insbesondere wenn du XGBoost verwendest, kann die Modellleistung erheblich verbessern und die Trainingszeit reduzieren. Es gibt mehrere Ansätze zur Feature-Auswahl, die du in Betracht ziehen kannst. Hier sind einige gängige Methoden, die effektiv mit XGBoost funktionieren:\n",
    "\n",
    "### 1. **XGBoost's Built-In Feature Importance**\n",
    "\n",
    "XGBoost bietet eingebaute Funktionen, um die Wichtigkeit der Features zu bewerten. Nach dem Trainieren deines Modells kannst du `model.feature_importances_` verwenden, um zu sehen, welche Features das Modell als am wichtigsten erachtet. Diese Methode ist schnell und einfach, aber sie gibt dir nur einen groben Überblick über die Feature-Wichtigkeit basierend auf der Anzahl der male, die ein Feature für das Splitting verwendet wurde, und der Verbesserung der Performance durch diese Splits.\n",
    "\n",
    "### 2. **Recursive Feature Elimination (RFE)**\n",
    "\n",
    "Die Recursive Feature Elimination (RFE) ist ein Verfahren, das ein Modell trainiert und die am wenigsten wichtigen Features schrittweise entfernt. Du kannst RFE mit XGBoost verwenden, indem du es mit dem RFE-Wrapper von `sklearn.feature_selection` kombinierst. Dieser Ansatz kann effektiver sein, um die optimale Feature-Set-Größe zu bestimmen, kann aber bei einer sehr großen Anzahl von Features zeitaufwendig sein.\n",
    "\n",
    "### 3. **SHAP Values**\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) bietet eine fundierte und konsistente Methode, um die Beiträge jedes Features zu jeder einzelnen Vorhersage zu erklären. SHAP-Werte können dir helfen zu verstehen, wie dein Modell Entscheidungen trifft, und gleichzeitig die wichtigsten Features identifizieren. Die Nutzung von SHAP mit XGBoost kann aufschlussreich sein, erfordert jedoch zusätzliche Berechnungen.\n",
    "\n",
    "### 4. **Feature Selection Using SelectFromModel**\n",
    "\n",
    "`SelectFromModel` ist ein Meta-Transformer von scikit-learn, der Features nach ihrem Gewicht selektiert. Du kannst `SelectFromModel` mit XGBoost verwenden, um Features mit geringer Wichtigkeit automatisch zu entfernen. Diese Methode ermöglicht es dir, ein Schwellenwert für die Feature-Wichtigkeit festzulegen und kann eine effiziente Art der Feature-Reduktion sein.\n",
    "\n",
    "### Beispielcode für Feature Importance mit XGBoost:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Angenommen, X_train und y_train sind bereits definiert\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Auswahl der Features basierend auf Gewichtungsschwelle\n",
    "thresholds = SelectFromModel(model, prefit=True)\n",
    "X_train_reduced = thresholds.transform(X_train)\n",
    "\n",
    "# X_train_reduced enthält nun nur noch die Features, die die Schwelle überschreiten\n",
    "```\n",
    "\n",
    "Jede dieser Methoden hat ihre eigenen Stärken und Einschränkungen. Es könnte sinnvoll sein, mit einigen davon zu experimentieren, um zu sehen, welche Methode die besten Ergebnisse für dein spezifisches Problem liefert. Beachte, dass die Effektivität der Feature-Auswahl stark von deinen Daten und dem spezifischen Problem, das du lösen möchtest, abhängt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
