{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for the 10 folds: [0.86666667 0.82022472 0.80898876 0.85393258 0.85393258 0.86516854\n",
      " 0.76404494 0.80898876 0.79775281 0.88764045]\n",
      "Average accuracy: 0.8327340823970036\n"
     ]
    }
   ],
   "source": [
    "# Überprüfe, ob im Testdatensatz 'Survived' fehlt und setze es auf -1, um die Struktur beizubehalten\n",
    "test['Survived'] = -1\n",
    "\n",
    "# Bereinige die Daten, falls nötig (z.B. fehlende Werte)\n",
    "# Für das Beispiel hier keine Bereinigung angenommen.\n",
    "\n",
    "train =  train.drop(['Ticket'], axis=1)\n",
    "test = test.drop(['Ticket'], axis=1)\n",
    "\n",
    "# Feature und Label definieren\n",
    "X_train = train.drop(['PassengerId', 'Survived','AverageSurvivalRate'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Modell vorbereiten\n",
    "model = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=7))\n",
    "\n",
    "# Cross-Validation\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy scores for the 10 folds: {scores}')\n",
    "print(f'Average accuracy: {scores.mean()}')\n",
    "\n",
    "# Modell mit dem gesamten Trainingsdatensatz trainieren\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen für Testdatensatz machen\n",
    "X_test = test.drop(['PassengerId', 'Survived','AverageSurvivalRate'], axis=1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Vorhersagen in den Testdatensatz einfügen und in CSV exportieren\n",
    "test['Survived'] = predictions\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path/'../csv/submission_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Modell zu optimieren, gibt es mehrere Ansätze, die du in Betracht ziehen kannst. Ein paar gängige Methoden umfassen die Auswahl von besseren Features, die Anwendung von Hyperparameter-Tuning und die Erprobung verschiedener Modelle. In diesem Beispiel konzentrieren wir uns auf das Hyperparameter-Tuning für den `RandomForestClassifier` mithilfe von `GridSearchCV` für eine systematische Durchsuchung eines Parameterbereichs.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter, die optimiert werden sollen\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV vorbereiten\n",
    "grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Suche ausführen\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter und Genauigkeit anzeigen\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "# Vorhersagen mit dem optimierten Modell machen\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Vorhersagen in den Testdatensatz einfügen und in CSV exportieren\n",
    "test['Survived'] = predictions\n",
    "result_optimized = test[['PassengerId', 'Survived']]\n",
    "result_optimized.to_csv('submission_test_optimized.csv', index=False)\n",
    "```\n",
    "\n",
    "Ein paar Punkte zur Berücksichtigung:\n",
    "\n",
    "- **Grid Search** kann sehr zeitaufwendig sein, insbesondere bei einer großen Anzahl von Hyperparametern und wenn du `n_jobs=-1` setzt, um alle Prozessoren zu verwenden. Überlege dir, ob du eine kleinere Auswahl an Werten für den ersten Durchlauf testen möchtest.\n",
    "- **Hyperparameter-Bereiche**: Die hier gewählten Werte (`n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`) sind gängige Startpunkte, aber je nach deinem spezifischen Datensatz könnten andere Werte oder zusätzliche Hyperparameter interessant sein.\n",
    "- **Modellauswahl**: RandomForest ist ein starker Baseline-Classifier, aber je nach den Eigenschaften deines Datensatzes könnten auch andere Modelle wie Gradient Boosting Machines (z.B. XGBoost, LightGBM) oder sogar einfache Modelle wie Logistische Regression besser performen.\n",
    "- **Feature Engineering**: Es könnte hilfreich sein, zusätzliches Feature Engineering durchzuführen, um die Modellleistung weiter zu verbessern. Dies könnte das Erstellen neuer Features, die Auswahl spezifischer Features oder das Anwenden von Feature Transformationen beinhalten.\n",
    "\n",
    "Durch das Anwenden dieser Optimierungstechniken solltest du in der Lage sein, die Leistung deines Modells weiter zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# # Daten laden\n",
    "# train = pd.read_csv(\"csv/train_engineered.csv\")\n",
    "# test = pd.read_csv(\"csv/test_engineered.csv\")\n",
    "\n",
    "# # Vorbereitung\n",
    "# test['Survived'] = -1\n",
    "# train = train.drop(['Ticket'], axis=1)\n",
    "# test = test.drop(['Ticket'], axis=1)\n",
    "\n",
    "# # Features und Labels definieren\n",
    "# X_train = train.drop(['PassengerId', 'Survived', 'AverageSurvivalRate'], axis=1)\n",
    "# y_train = train['Survived']\n",
    "\n",
    "# # Modell vorbereiten mit einem geänderten Zufallszustand für Konsistenz\n",
    "# model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=7))\n",
    "\n",
    "# # Hyperparameter für GridSearch\n",
    "# param_grid = {\n",
    "#     'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "#     'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "#     'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "#     'randomforestclassifier__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # GridSearchCV vorbereiten\n",
    "# cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# # Suche ausführen\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Beste Parameter und Genauigkeit anzeigen\n",
    "# print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "# # Vorhersagen mit dem optimierten Modell machen\n",
    "# X_test = test.drop(['PassengerId', 'Survived', 'AverageSurvivalRate'], axis=1)\n",
    "# predictions = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# # Vorhersagen in den Testdatensatz einfügen und in CSV exportieren\n",
    "# test['Survived'] = predictions\n",
    "# result_optimized = test[['PassengerId', 'Survived']]\n",
    "# result_optimized.to_csv('submission_test_optimized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Daten laden\n",
    "# train = pd.read_csv(\"csv/train_engineered.csv\")\n",
    "# test = pd.read_csv(\"csv/test_engineered.csv\")\n",
    "\n",
    "# # Vorbereitung\n",
    "# test['Survived'] = -1\n",
    "# train = train.drop(['Ticket'], axis=1)\n",
    "# test = test.drop(['Ticket'], axis=1)\n",
    "\n",
    "# # Features und Labels definieren\n",
    "# X_train = train.drop(['PassengerId', 'Survived', 'AverageSurvivalRate'], axis=1)\n",
    "# y_train = train['Survived']\n",
    "\n",
    "# # Modell Pipeline vorbereiten mit PCA\n",
    "# model = make_pipeline(\n",
    "#     StandardScaler(),\n",
    "#     PCA(n_components=0.95),  # Behalte 95% der Varianz\n",
    "#     RandomForestClassifier(random_state=7)\n",
    "# )\n",
    "\n",
    "# # Hyperparameter für GridSearch, jetzt mit PCA Komponenten\n",
    "# param_grid = {\n",
    "#     'pca__n_components': [0.80, 0.85, 0.90, 0.95],  # Unterschiedliche Varianzanteile, die beibehalten werden sollen\n",
    "#     'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "#     'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "#     'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "#     'randomforestclassifier__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # GridSearchCV vorbereiten\n",
    "# cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# # Suche ausführen\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Beste Parameter und Genauigkeit anzeigen\n",
    "# print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "# # Vorhersagen mit dem optimierten Modell machen\n",
    "# X_test = test.drop(['PassengerId', 'Survived', 'AverageSurvivalRate'], axis=1)\n",
    "# predictions = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# # Vorhersagen in den Testdatensatz einfügen und in CSV exportieren\n",
    "# test['Survived'] = predictions\n",
    "# result_optimized = test[['PassengerId', 'Survived']]\n",
    "# result_optimized.to_csv('submission_test_optimized.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasience_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
