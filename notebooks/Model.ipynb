{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell:  XGBoost-Klassifikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "418\n",
      "Accuracy scores for the 10 folds: [0.88333333 0.85       0.91666667 0.9        0.9        0.83333333\n",
      " 0.93220339 0.96610169 0.91525424 0.86440678 0.81355932 0.84745763\n",
      " 0.84745763 0.88135593 0.93220339]\n",
      "Average accuracy: 0.8855555555555554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_path = Path.cwd()\n",
    "\n",
    "# Pfad zur CSV-Datei das Später erstellt wird\n",
    "train = pd.read_csv(data_path/'../csv/feature_data/train_engineered.csv')\n",
    "test = pd.read_csv(data_path/'../csv/feature_data/test_engineered.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# Angenommen, deine Trainings- und Testdatensätze wurden bereits geladen in `train` und `test`\n",
    "\n",
    "# Setze 'Survived' im Testdatensatz auf -1, um die Struktur beizubehalten\n",
    "test['Survived'] = -1\n",
    "\n",
    "# Entferne die 'Ticket' Spalte aus beiden Datensätzen\n",
    "# train = train.drop(['Ticket'], axis=1)\n",
    "# test = test.drop(['Ticket'], axis=1)\n",
    "\n",
    "# Definiere Features und Label\n",
    "X_train = train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vorbereitung des Modells\n",
    "# Hier verwenden wir XGBClassifier direkt, da XGBoost seine eigene Methode zur Handhabung von Feature-Skalierung hat,\n",
    "# und es ist nicht zwingend notwendig, StandardScaler in der Pipeline zu verwenden.\n",
    "model = XGBClassifier(n_estimators=100, random_state=7, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Cross-Validation\n",
    "cv = KFold(n_splits=15, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy scores for the 10 folds: {scores}')\n",
    "print(f'Average accuracy: {scores.mean()}')\n",
    "\n",
    "# Trainiere das Modell mit dem gesamten Trainingsdatensatz\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen für den Testdatensatz\n",
    "X_test = test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Füge Vorhersagen in den Testdatensatz ein und exportiere in CSV\n",
    "test['Survived'] = predictions\n",
    "result = test[['PassengerId', 'Survived']]\n",
    "result.to_csv(data_path / '../csv/submission_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Auswahl der wichtigsten Features für ein Modell, insbesondere wenn du XGBoost verwendest, kann die Modellleistung erheblich verbessern und die Trainingszeit reduzieren. Es gibt mehrere Ansätze zur Feature-Auswahl, die du in Betracht ziehen kannst. Hier sind einige gängige Methoden, die effektiv mit XGBoost funktionieren:\n",
    "\n",
    "### 1. **XGBoost's Built-In Feature Importance**\n",
    "\n",
    "XGBoost bietet eingebaute Funktionen, um die Wichtigkeit der Features zu bewerten. Nach dem Trainieren deines Modells kannst du `model.feature_importances_` verwenden, um zu sehen, welche Features das Modell als am wichtigsten erachtet. Diese Methode ist schnell und einfach, aber sie gibt dir nur einen groben Überblick über die Feature-Wichtigkeit basierend auf der Anzahl der male, die ein Feature für das Splitting verwendet wurde, und der Verbesserung der Performance durch diese Splits.\n",
    "\n",
    "### 2. **Recursive Feature Elimination (RFE)**\n",
    "\n",
    "Die Recursive Feature Elimination (RFE) ist ein Verfahren, das ein Modell trainiert und die am wenigsten wichtigen Features schrittweise entfernt. Du kannst RFE mit XGBoost verwenden, indem du es mit dem RFE-Wrapper von `sklearn.feature_selection` kombinierst. Dieser Ansatz kann effektiver sein, um die optimale Feature-Set-Größe zu bestimmen, kann aber bei einer sehr großen Anzahl von Features zeitaufwendig sein.\n",
    "\n",
    "### 3. **SHAP Values**\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) bietet eine fundierte und konsistente Methode, um die Beiträge jedes Features zu jeder einzelnen Vorhersage zu erklären. SHAP-Werte können dir helfen zu verstehen, wie dein Modell Entscheidungen trifft, und gleichzeitig die wichtigsten Features identifizieren. Die Nutzung von SHAP mit XGBoost kann aufschlussreich sein, erfordert jedoch zusätzliche Berechnungen.\n",
    "\n",
    "### 4. **Feature Selection Using SelectFromModel**\n",
    "\n",
    "`SelectFromModel` ist ein Meta-Transformer von scikit-learn, der Features nach ihrem Gewicht selektiert. Du kannst `SelectFromModel` mit XGBoost verwenden, um Features mit geringer Wichtigkeit automatisch zu entfernen. Diese Methode ermöglicht es dir, ein Schwellenwert für die Feature-Wichtigkeit festzulegen und kann eine effiziente Art der Feature-Reduktion sein.\n",
    "\n",
    "### Beispielcode für Feature Importance mit XGBoost:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Angenommen, X_train und y_train sind bereits definiert\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Auswahl der Features basierend auf Gewichtungsschwelle\n",
    "thresholds = SelectFromModel(model, prefit=True)\n",
    "X_train_reduced = thresholds.transform(X_train)\n",
    "\n",
    "# X_train_reduced enthält nun nur noch die Features, die die Schwelle überschreiten\n",
    "```\n",
    "\n",
    "Jede dieser Methoden hat ihre eigenen Stärken und Einschränkungen. Es könnte sinnvoll sein, mit einigen davon zu experimentieren, um zu sehen, welche Methode die besten Ergebnisse für dein spezifisches Problem liefert. Beachte, dass die Effektivität der Feature-Auswahl stark von deinen Daten und dem spezifischen Problem, das du lösen möchtest, abhängt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasience_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
